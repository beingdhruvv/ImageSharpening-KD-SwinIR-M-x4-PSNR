# Training History & Tracking

This folder stores all textual logs related to the training of the student Mini-UNet model, including loss values across epochs, checkpoints, and metadata for reproducibility.

It serves as a historical record of your training sessions and supports tracking model performance across multiple configurations.

---

## Contents

| File Name                      | Description                                                |
| ------------------------------ | ---------------------------------------------------------- |
| `student_kd_training_log.txt`  | Logs loss per epoch during knowledge distillation training |
| `train_student.log` (optional) | Logs loss for L1-only training runs                        |
| `train_distill_vgg.log` (opt)  | Logs training with perceptual loss (if enabled)            |

> All logs are auto-generated by their respective training scripts in the `/code/` directory.

---

## Logging Format Example

Each log file typically includes lines like:

```
Epoch 1 - Loss: 0.2547
Epoch 2 - Loss: 0.1497
...
Epoch 25 - Loss: 0.0546
```

Optionally, additional information such as validation performance or learning rate may be included if extended.

---

## Tips for Usage

* Use logs to track convergence behavior
* Useful for comparing training curves across architectures or loss functions
* You can plot the losses using matplotlib or seaborn
* Helps in debugging underfitting or overfitting issues

---

## Where Logs Are Created

Logs are automatically saved to:

```
/content/drive/MyDrive/ImageSharpening_KD/logs/
```

by the training scripts in `/code/`. You can modify file names inside the scripts if needed.

---

## Notes

* Logs are lightweight and recommended to include in GitHub for traceability
* No sensitive data is stored
* Format: plain `.txt`

---

## License

This folder contains text-only metadata and falls under the MIT License of the project. Safe to share, archive, or publish.

---

