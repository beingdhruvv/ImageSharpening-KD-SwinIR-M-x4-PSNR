# Image Sharpening using Knowledge Distillation (SwinIR Teacher)

This repository contains a complete end-to-end pipeline for **image sharpening (deblurring)** using **knowledge distillation**. A powerful SwinIR-M model acts as the **teacher**, while a lightweight **Mini-UNet** is trained as the **student** to mimic its behavior. The student model is capable of real-time performance while retaining strong SSIM/PSNR quality.

---

## Project Summary

| Component      | Details                                                              |
| -------------- | -------------------------------------------------------------------- |
| Teacher     | SwinIR-M PSNR-optimized model (`x4`, trained on BSRGAN degradations) |
| Student     | Mini-UNet (2â€“3 stages, skip connections, Conv+BN+ReLU blocks)        |
| Patch Size | 512Ã—512                                                              |
| Goal        | SSIM â‰¥ 0.90 on benchmark/test via distillation training              |
| Platform    | Google Colab + Google Drive (Free GPU Tier)                          |

---

## Folder Structure (with placeholders)

This project is structured to keep the dataset excluded, but its structure preserved with `.gitkeep` or `README.md` placeholders:

```
ImageSharpening_KD/
â”œâ”€â”€ code/                        # All training, testing, model scripts
â”‚   â”œâ”€â”€ student_model.py         # Mini-UNet architecture
â”‚   â”œâ”€â”€ dataset_loader.py        # Dataset class with blurry, sharp, teacher triplets
â”‚   â”œâ”€â”€ train_student.py         # L1-only training
â”‚   â”œâ”€â”€ train_distill.py         # KD training (L1 + teacher output loss)
â”‚   â”œâ”€â”€ train_distill_vgg.py     # Optional: adds perceptual (VGG) loss
â”‚
â”œâ”€â”€ models/                      # Teacher + student weights
â”‚   â”œâ”€â”€ swinir_teacher/          # SwinIR pretrained model (README/placeholder only)
â”‚   â”‚   â””â”€â”€ README.md            # Path/URL reference to model (not included)
â”‚   â””â”€â”€ student_kd.pt            # Final trained student model (if <100MB)
â”‚
â”‚
â”œâ”€â”€ logs/                        # Training logs
â”‚   â””â”€â”€ student_kd_training_log.txt
â”‚
â”œâ”€â”€ data/                        # [Excluded from repo] Full structure maintained
â”‚   â”œâ”€â”€ whole_dataset/.gitkeep   # Original DIV2K images (source only)
â”‚   â”œâ”€â”€ blurry/train/train/.gitkeep
â”‚   â”œâ”€â”€ blurry/train/test/.gitkeep
â”‚   â”œâ”€â”€ blurry/benchmark/.gitkeep
â”‚   â”œâ”€â”€ sharp/train/train/.gitkeep
â”‚   â”œâ”€â”€ sharp/train/test/.gitkeep
â”‚   â””â”€â”€ sharp/benchmark/.gitkeep
â”‚
â”œâ”€â”€ ISKD - SwinIR-M_x4_PSNR.ipynb  # Final working Colab notebook (end-to-end)
â””â”€â”€ README.md                     # This documentation
```

---

## Workflow Overview

### Dataset Preparation

* Source: `DIV2K_train_HR/` â†’ saved in `/data/whole_dataset/`
* 512Ã—512 patches created with non-overlapping stride
* Blurry patches created by downscaling + upscaling
* Splits:

  * **90% Training (â†’ 80% train, 20% test)**
  * **10% Benchmark**
* Total patches: `~55,916`

### Teacher Inference (SwinIR)

* Model: `003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_PSNR.pth`
* Inference over all blurry patches
* Output stored in:

  * `/outputs/teacher_output/train/train/`
  * `/outputs/teacher_output/train/test/`
  * `/outputs/teacher_output/benchmark/`

### Student Training

* Architecture: Mini-UNet (2â€“3 downsampling blocks + skip connections)
* Loss Functions:

  * `loss_total = Î± * L1(output, GT_sharp) + Î² * MSE(output, Teacher_pred)`
  * Optional: add `+ Î³ * VGG(output, GT_sharp)`
* Training runs for 25â€“75 epochs depending on image count

### Evaluation

* SSIM + PSNR computed on all sets
* External test support (3â€“5 real-world blurry/sharp images)

---

## Results

| Dataset Split  | SSIM   | PSNR    |
| -------------- | ------ | ------- |
| Train          | 0.72   | 24.6 dB |
| Test           | 0.78   | 28.5 dB |
| Benchmark      | 0.75   | 26.5 dB |
| External (avg) | \~0.71 | -       |

> Note: SSIM calculated on full RGB uint8 images using scikit-image

---

## External Image Testing

* Test real-world image pairs via `test_external_images.py`
* Folder structure:

```
outputs/external_test/
â”œâ”€â”€ input_blurry/
â”œâ”€â”€ ground_truth/
â”œâ”€â”€ predicted/
â””â”€â”€ ssim_results.txt
```

* Auto-resumes and skips already tested images
* Displays visual comparison and prints SSIM score per image

---

## ðŸ›  Requirements

This project is designed to run entirely in **Google Colab**. If running locally, install:

```bash
pip install torch torchvision timm einops yacs scikit-image opencv-python tqdm matplotlib
```

Optional:

* VGG-based perceptual loss (torchvision >= 0.15 recommended)

---

## â–¶How to Use

### Run in Colab

> Open the notebook:

```bash
ISKD - SwinIR-M_x4_PSNR.ipynb
```

Steps included:

1. Mount Drive
2. Preprocess & split dataset
3. Run SwinIR inference (teacher)
4. Train Mini-UNet (student)
5. Evaluate and visualize results

### Manual Script Execution

> Located in `/code/`

* `train_student.py`: L1 only
* `train_distill.py`: L1 + distillation loss
* `train_distill_vgg.py`: Adds VGG perceptual loss
* `test_external_images.py`: Evaluate 3â€“5 blurry-sharp pairs

---

## Model Weights

| Model               | Location                  | Size (approx.) |
| ------------------- | ------------------------- | -------------- |
| SwinIR-M (PSNR)     | `/models/swinir_teacher/` | \~47 MB        |
| Student (Mini-UNet) | `/models/student_kd.pt`   | \~1â€“5 MB       |

---

## âš Notes

* Dataset is excluded from this repo (too large) â†’ use placeholders
* To recreate patches, refer to the preprocessing script in the notebook
* External test pairs should be visually and resolution matched

---

## Author & Credits

**Project Lead**: [Dhruv](https://github.com/beingdhruvv)
**Inspired by**:

* [SwinIR](https://github.com/JingyunLiang/SwinIR)
* [DIV2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/)

---

## ðŸ“œ License

MIT License. This project is open-source and free for academic, educational, and research use.

---

> For questions, raise an issue or submit a PR. Contributions welcome!
